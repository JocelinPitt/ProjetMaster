{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Bert - Multi-Input .ipynb",
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyPiD4rt+jpXD8fFwgg/mt1X"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This Jupyter Notebook is the last online version that i've tried.\n",
    "The goal here is to create a Keras layer that will call BERT.\n",
    "An error still isn't resolved here, the \"with tf.io.gfile\" in the last cells.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8O9aTGv2xnq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635202512729,
     "user_tz": -120,
     "elapsed": 3060,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "07c12df0-6ecd-4e00-9f7e-a7efc595dffd"
   },
   "source": [
    "!pip install bert-tensorflow"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow) (1.15.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfjL_50bodJq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635202518736,
     "user_tz": -120,
     "elapsed": 6012,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "c4aeb73a-f571-4652-e26d-6af895f88dd4"
   },
   "source": [
    "!pip install bert-tensorflow==1.0.1\n",
    "!pip install tensorflow==2.0.0"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: bert-tensorflow==1.0.1 in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from bert-tensorflow==1.0.1) (1.15.0)\n",
      "Requirement already satisfied: tensorflow==2.0.0 in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.37.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.3.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.19.5)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (2.0.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.12.1)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.41.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.17.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (3.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (57.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0.0) (1.5.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.6.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ADSP81OW1Ywh"
   },
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "#utiliser \n",
    "\n",
    "#résolution possible du porblème dans main\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import bert as bert\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import backend as K"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AQromVC2LWm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635202521610,
     "user_tz": -120,
     "elapsed": 1199,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "c816cd21-e9e9-4913-96de-bc351db5431e"
   },
   "source": [
    "# get the data\n",
    "# import dependencies\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULVkIzIn4Ius"
   },
   "source": [
    "# Initialize session"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R5AJyeLK1dXq"
   },
   "source": [
    "sess = tf.compat.v1.Session()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzCtxn9k4PCG"
   },
   "source": [
    "## Load all files from a directory in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uu7GoZsFJaLl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635202521611,
     "user_tz": -120,
     "elapsed": 24,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "90180f01-46c0-4601-ef5d-7115d2739ce1"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-vXycMO4eGd"
   },
   "source": [
    "###exemple dataset -> redo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CrlA6MHx1fts"
   },
   "source": [
    "''''def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)'''"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0U9fSMgy1hXs"
   },
   "source": [
    "'''# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)'''"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HITQ7NQY1i6X"
   },
   "source": [
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "        fname=\"aclImdb.tar.gz\",\n",
    "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "        extract=True,\n",
    "    )\n",
    "\n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_df, test_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZA_75or4lkQ"
   },
   "source": [
    "## BertClass and Converter"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xzX68IR-1ppW"
   },
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b38AXzVf1rmi"
   },
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vM0yk56v1uAK"
   },
   "source": [
    "def create_tokenizer_from_hub_module(bert_path):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module = hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [tokenization_info[\"vocab_file\"], tokenization_info[\"do_lower_case\"]]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kbyselXy1wqZ"
   },
   "source": [
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ttzn6jIA1ybD"
   },
   "source": [
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Tk7mGZrt10ZK"
   },
   "source": [
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PCQNgF8d2AJ7"
   },
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"mean\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = False\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spL81siTL8gP"
   },
   "source": [
    "## Model Build"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H9oH64vZ2DUq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1637101515661,
     "user_tz": -60,
     "elapsed": 3,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    }
   },
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length,num_LIWC):\n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    bert_output = BertLayer(n_fine_tune_layers=3)(bert_inputs)\n",
    "\n",
    "    LIWC_input = tf.keras.layers.Input(shape=(num_LIWC,), name=\"LIWC\")\n",
    "\n",
    "    concat = tf.keras.layers.concatenate([bert_output, tags_input])\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation=\"relu\")(concat)\n",
    "    pred = tf.keras.layers.Dense(16, activation=\"sigmoid\")(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[bert_inputs,LIWC_input], outputs=pred)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxlXlTIpMCuk"
   },
   "source": [
    "## Init Vars and Main"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZFuiwJe02FDq"
   },
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZoODfoYJ2IFk"
   },
   "source": [
    "def main(train_text, train_label, test_text, test_label, train_liwcs, test_liwcs):\n",
    "    # Params for bert model and tokenization\n",
    "    bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "    max_seq_length = 256\n",
    "\n",
    "\n",
    "    # Instantiate tokenizer\n",
    "    tokenizer = create_tokenizer_from_hub_module(bert_path)\n",
    "\n",
    "    # Convert data to InputExample format\n",
    "    train_examples = convert_text_to_examples(train_text, train_label)\n",
    "    test_examples = convert_text_to_examples(test_text, test_label)\n",
    "\n",
    "    # Set LIWCS\n",
    "    num_LIWCs = 93\n",
    "\n",
    "    # Convert to features\n",
    "    (\n",
    "        train_input_ids,\n",
    "        train_input_masks,\n",
    "        train_segment_ids,\n",
    "        train_labels,\n",
    "    ) = convert_examples_to_features(\n",
    "        tokenizer, train_examples, max_seq_length=max_seq_length\n",
    "    )\n",
    "    (\n",
    "        test_input_ids,\n",
    "        test_input_masks,\n",
    "        test_segment_ids,\n",
    "        test_labels,\n",
    "    ) = convert_examples_to_features(\n",
    "        tokenizer, test_examples, max_seq_length=max_seq_length\n",
    "    )\n",
    "\n",
    "    model = build_model(max_seq_length, num_LIWCs)\n",
    "\n",
    "    # Instantiate variables\n",
    "    initialize_vars(sess)\n",
    "\n",
    "    model.fit(\n",
    "        [train_input_ids, train_input_masks, train_segment_ids, train_liwcs],\n",
    "        train_labels,\n",
    "        validation_data=(\n",
    "            [test_input_ids, test_input_masks, test_segment_ids, test_liwcs],\n",
    "            test_labels,\n",
    "        ),\n",
    "        epochs=1,\n",
    "        batch_size=32,\n",
    "    )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0rBSzwSMSIh"
   },
   "source": [
    "#Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrEe57k5MUhd"
   },
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aWhuHZIH2V2Q"
   },
   "source": [
    "# set random seed\n",
    "np.random.seed(42)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TD3xQ2m9JliD"
   },
   "source": [
    "#DATA = pd.read_csv('/content/drive/MyDrive/DATA_full.csv')\n",
    "MBTI = pd.read_csv('/content/drive/My Drive/MBTI_full_aug.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QbscEVKOjt0I"
   },
   "source": [
    "#DATA = DATA.drop(['Unnamed: 0', 'Segment'], axis='columns')\n",
    "MBTI = MBTI.drop('Unnamed: 0', axis='columns')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_huY1-AzPrOh"
   },
   "source": [
    "LIWCs = MBTI[MBTI.columns[2:-1]]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ehdKX8GcK2yp"
   },
   "source": [
    "df = MBTI[['posts','type']]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = pd.concat([df, df.type.astype('str').str.get_dummies()], axis=1, sort=False)\n",
    "df.drop('type', axis=1, inplace=True)\n",
    "df.rename(columns={'posts':'text'}, inplace=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SwaqyENYfdww"
   },
   "source": [
    "df.drop('text', axis=1, inplace=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FaPHhy-UfvYB"
   },
   "source": [
    "Labels = df[df.columns[-1:]]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "-Wz5dEc_2YrU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635202523731,
     "user_tz": -120,
     "elapsed": 27,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "0bfbf84b-82c7-413c-e344-ca04f16977cf"
   },
   "source": [
    "'''# add the corpus\n",
    "with open('labelled_data.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader) # skip header\n",
    "    labelled_data = [tuple(row) for row in reader]'''"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"# add the corpus\\nwith open('labelled_data.csv', newline='') as f:\\n    reader = csv.reader(f)\\n    next(reader) # skip header\\n    labelled_data = [tuple(row) for row in reader]\""
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5scHQT-N2bcH",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635202523732,
     "user_tz": -120,
     "elapsed": 27,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "654fdfe7-6e95-4be0-bda2-ed90f65d00b9"
   },
   "source": [
    "'''# generate a balanced data set\n",
    "# separate into positive and negative comments\n",
    "positives = []\n",
    "for tup in labelled_data:\n",
    "    if tup[1] == \"positive\":\n",
    "        positives.append(tup)\n",
    "\n",
    "negatives = []\n",
    "for tup in labelled_data:\n",
    "    if tup[1] == \"negative\":\n",
    "        negatives.append(tup)'''"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'# generate a balanced data set\\n# separate into positive and negative comments\\npositives = []\\nfor tup in labelled_data:\\n    if tup[1] == \"positive\":\\n        positives.append(tup)\\n\\nnegatives = []\\nfor tup in labelled_data:\\n    if tup[1] == \"negative\":\\n        negatives.append(tup)'"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2ZU8aRDRQsVZ"
   },
   "source": [
    "#out = [df[x].value_counts()[1] for x in Labels_list]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kmwW7mL8SBsG"
   },
   "source": [
    "Labels_list = list(df.columns)[1:]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yjgHefK_UkcM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635202523734,
     "user_tz": -120,
     "elapsed": 26,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "218f90ca-a536-4d21-9451-590504e4a961"
   },
   "source": [
    "'''# base the number of samples on the lesser of two category counts    \n",
    "import random\n",
    "sample_count = min(len(positives), len(negatives))\n",
    "balanced_positives = random.sample(positives, sample_count)\n",
    "balanced_negatives = random.sample(negatives, sample_count)\n",
    "balanced_data = balanced_positives + balanced_negatives\n",
    "# shuffle the data\n",
    "random.shuffle(balanced_data)\n",
    "# convert to a dataframe and label the columns\n",
    "Corpus = pd.DataFrame(balanced_data)    \n",
    "Corpus.columns = ['text', 'label']'''"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"# base the number of samples on the lesser of two category counts    \\nimport random\\nsample_count = min(len(positives), len(negatives))\\nbalanced_positives = random.sample(positives, sample_count)\\nbalanced_negatives = random.sample(negatives, sample_count)\\nbalanced_data = balanced_positives + balanced_negatives\\n# shuffle the data\\nrandom.shuffle(balanced_data)\\n# convert to a dataframe and label the columns\\nCorpus = pd.DataFrame(balanced_data)    \\nCorpus.columns = ['text', 'label']\""
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoNmwNwbPN6H"
   },
   "source": [
    "## go on"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JRsWYC8D2g7G",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635202523734,
     "user_tz": -120,
     "elapsed": 22,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "231b2a9c-a7c5-4137-cf57-2019c9d8ad1b"
   },
   "source": [
    "'''\n",
    "# Step - a : Remove blank rows if any.\n",
    "MBTI['posts'].dropna(inplace=True)\n",
    "# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "MBTI['posts'] = [entry.lower() for entry in MBTI['posts']]\n",
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "MBTI['posts'] = [word_tokenize(entry) for entry in MBTI['posts']]\n",
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV'''"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n# Step - a : Remove blank rows if any.\\nMBTI['posts'].dropna(inplace=True)\\n# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\\nMBTI['posts'] = [entry.lower() for entry in MBTI['posts']]\\n# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\\nMBTI['posts'] = [word_tokenize(entry) for entry in MBTI['posts']]\\n# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\\n# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\\ntag_map = defaultdict(lambda : wn.NOUN)\\ntag_map['J'] = wn.ADJ\\ntag_map['V'] = wn.VERB\\ntag_map['R'] = wn.ADV\""
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OS30cT312ji2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635202523735,
     "user_tz": -120,
     "elapsed": 22,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "73558a80-7b87-46a8-895b-b150e32ccc65"
   },
   "source": [
    "'''for index,entry in enumerate(MBTI['posts']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    MBTI.loc[index,'text_final'] = str(Final_words)'''"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"for index,entry in enumerate(MBTI['posts']):\\n    # Declaring Empty List to store the words that follow the rules for this step\\n    Final_words = []\\n    # Initializing WordNetLemmatizer()\\n    word_Lemmatized = WordNetLemmatizer()\\n    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\\n    for word, tag in pos_tag(entry):\\n        # Below condition is to check for Stop words and consider only alphabets\\n        if word not in stopwords.words('english') and word.isalpha():\\n            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\\n            Final_words.append(word_Final)\\n    # The final processed set of words for each iteration will be stored in 'text_final'\\n    MBTI.loc[index,'text_final'] = str(Final_words)\""
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6cK-JWL3ewzR"
   },
   "source": [
    "#MBTI.to_csv('/content/drive/My Drive/MBTI_full_aug.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ti8HHF47nrqH"
   },
   "source": [
    "MBTI = pd.read_csv('/content/drive/My Drive/MBTI_full_aug.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "TWBdQzg6n1ee",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635202525811,
     "user_tz": -120,
     "elapsed": 9,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "8862d809-4196-4553-de6b-6dba85cc5b50"
   },
   "source": [
    "MBTI.head()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>Tone</th>\n",
       "      <th>WPS</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>Dic</th>\n",
       "      <th>function</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>shehe</th>\n",
       "      <th>they</th>\n",
       "      <th>ipron</th>\n",
       "      <th>article</th>\n",
       "      <th>prep</th>\n",
       "      <th>auxverb</th>\n",
       "      <th>adverb</th>\n",
       "      <th>conj</th>\n",
       "      <th>negate</th>\n",
       "      <th>verb</th>\n",
       "      <th>adj</th>\n",
       "      <th>compare</th>\n",
       "      <th>interrog</th>\n",
       "      <th>number</th>\n",
       "      <th>quant</th>\n",
       "      <th>affect</th>\n",
       "      <th>posemo</th>\n",
       "      <th>negemo</th>\n",
       "      <th>anx</th>\n",
       "      <th>anger</th>\n",
       "      <th>sad</th>\n",
       "      <th>social</th>\n",
       "      <th>family</th>\n",
       "      <th>...</th>\n",
       "      <th>sexual</th>\n",
       "      <th>ingest</th>\n",
       "      <th>drives</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>achieve</th>\n",
       "      <th>power</th>\n",
       "      <th>reward</th>\n",
       "      <th>risk</th>\n",
       "      <th>focuspast</th>\n",
       "      <th>focuspresent</th>\n",
       "      <th>focusfuture</th>\n",
       "      <th>relativ</th>\n",
       "      <th>motion</th>\n",
       "      <th>space</th>\n",
       "      <th>time</th>\n",
       "      <th>work</th>\n",
       "      <th>leisure</th>\n",
       "      <th>home</th>\n",
       "      <th>money</th>\n",
       "      <th>relig</th>\n",
       "      <th>death</th>\n",
       "      <th>informal</th>\n",
       "      <th>swear</th>\n",
       "      <th>netspeak</th>\n",
       "      <th>assent</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>[\"'http\", ':', '//www.youtube.com/watch', '?',...</td>\n",
       "      <td>599</td>\n",
       "      <td>82.33</td>\n",
       "      <td>69.77</td>\n",
       "      <td>55.76</td>\n",
       "      <td>75.36</td>\n",
       "      <td>11.98</td>\n",
       "      <td>16.19</td>\n",
       "      <td>81.47</td>\n",
       "      <td>46.74</td>\n",
       "      <td>13.02</td>\n",
       "      <td>8.01</td>\n",
       "      <td>4.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.67</td>\n",
       "      <td>5.01</td>\n",
       "      <td>6.68</td>\n",
       "      <td>13.69</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.34</td>\n",
       "      <td>5.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.68</td>\n",
       "      <td>5.51</td>\n",
       "      <td>3.34</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.51</td>\n",
       "      <td>5.34</td>\n",
       "      <td>4.01</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>9.35</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>7.01</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.50</td>\n",
       "      <td>6.51</td>\n",
       "      <td>0.67</td>\n",
       "      <td>15.53</td>\n",
       "      <td>1.50</td>\n",
       "      <td>8.18</td>\n",
       "      <td>6.01</td>\n",
       "      <td>1.84</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.36</td>\n",
       "      <td>13.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>['intj', 'moment', 'http', 'sportscenter', 'to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ENTP</td>\n",
       "      <td>[\"'i\", \"'m\", 'finding', 'the', 'lack', 'of', '...</td>\n",
       "      <td>1262</td>\n",
       "      <td>56.01</td>\n",
       "      <td>48.42</td>\n",
       "      <td>65.89</td>\n",
       "      <td>55.66</td>\n",
       "      <td>10.79</td>\n",
       "      <td>12.20</td>\n",
       "      <td>80.59</td>\n",
       "      <td>51.58</td>\n",
       "      <td>18.46</td>\n",
       "      <td>14.10</td>\n",
       "      <td>9.43</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.03</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5.78</td>\n",
       "      <td>12.28</td>\n",
       "      <td>6.02</td>\n",
       "      <td>4.60</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.87</td>\n",
       "      <td>13.47</td>\n",
       "      <td>5.71</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.74</td>\n",
       "      <td>6.34</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.63</td>\n",
       "      <td>9.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.05</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.69</td>\n",
       "      <td>9.27</td>\n",
       "      <td>0.79</td>\n",
       "      <td>11.25</td>\n",
       "      <td>1.35</td>\n",
       "      <td>6.18</td>\n",
       "      <td>3.88</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.42</td>\n",
       "      <td>12.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>['find', 'lack', 'post', 'bore', 'position', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>INTP</td>\n",
       "      <td>[\"'good\", 'one', '_____', 'https', ':', '//www...</td>\n",
       "      <td>900</td>\n",
       "      <td>41.07</td>\n",
       "      <td>52.22</td>\n",
       "      <td>52.96</td>\n",
       "      <td>75.29</td>\n",
       "      <td>15.79</td>\n",
       "      <td>16.22</td>\n",
       "      <td>82.44</td>\n",
       "      <td>50.78</td>\n",
       "      <td>16.89</td>\n",
       "      <td>8.44</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.22</td>\n",
       "      <td>8.33</td>\n",
       "      <td>3.56</td>\n",
       "      <td>11.22</td>\n",
       "      <td>6.89</td>\n",
       "      <td>8.00</td>\n",
       "      <td>5.67</td>\n",
       "      <td>1.00</td>\n",
       "      <td>13.22</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.33</td>\n",
       "      <td>3.22</td>\n",
       "      <td>1.78</td>\n",
       "      <td>7.33</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.22</td>\n",
       "      <td>7.22</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>6.33</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>2.44</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.11</td>\n",
       "      <td>12.33</td>\n",
       "      <td>0.56</td>\n",
       "      <td>5.11</td>\n",
       "      <td>6.44</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.22</td>\n",
       "      <td>11.78</td>\n",
       "      <td>9.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>['one', 'http', 'course', 'say', 'know', 'bles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>[\"'dear\", 'intp', ',', 'i', 'enjoyed', 'our', ...</td>\n",
       "      <td>1162</td>\n",
       "      <td>41.54</td>\n",
       "      <td>52.07</td>\n",
       "      <td>55.11</td>\n",
       "      <td>64.91</td>\n",
       "      <td>11.50</td>\n",
       "      <td>15.06</td>\n",
       "      <td>83.48</td>\n",
       "      <td>53.96</td>\n",
       "      <td>18.67</td>\n",
       "      <td>12.65</td>\n",
       "      <td>7.23</td>\n",
       "      <td>1.12</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.43</td>\n",
       "      <td>6.02</td>\n",
       "      <td>5.42</td>\n",
       "      <td>11.70</td>\n",
       "      <td>6.97</td>\n",
       "      <td>6.97</td>\n",
       "      <td>6.54</td>\n",
       "      <td>1.46</td>\n",
       "      <td>14.03</td>\n",
       "      <td>5.85</td>\n",
       "      <td>3.36</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.07</td>\n",
       "      <td>4.04</td>\n",
       "      <td>5.34</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.17</td>\n",
       "      <td>10.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.42</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3.36</td>\n",
       "      <td>8.95</td>\n",
       "      <td>0.69</td>\n",
       "      <td>8.35</td>\n",
       "      <td>0.60</td>\n",
       "      <td>4.82</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.77</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>['intp', 'enjoy', 'conversation', 'day', 'esot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ENTJ</td>\n",
       "      <td>[\"'you\", \"'re\", 'fired.|||that', \"'s\", 'anothe...</td>\n",
       "      <td>1043</td>\n",
       "      <td>41.29</td>\n",
       "      <td>60.23</td>\n",
       "      <td>28.77</td>\n",
       "      <td>52.76</td>\n",
       "      <td>10.75</td>\n",
       "      <td>15.15</td>\n",
       "      <td>85.71</td>\n",
       "      <td>53.40</td>\n",
       "      <td>17.55</td>\n",
       "      <td>10.26</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.73</td>\n",
       "      <td>7.29</td>\n",
       "      <td>5.08</td>\n",
       "      <td>11.03</td>\n",
       "      <td>9.97</td>\n",
       "      <td>4.79</td>\n",
       "      <td>5.66</td>\n",
       "      <td>1.73</td>\n",
       "      <td>16.97</td>\n",
       "      <td>5.94</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.40</td>\n",
       "      <td>6.90</td>\n",
       "      <td>4.12</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.19</td>\n",
       "      <td>11.79</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.34</td>\n",
       "      <td>8.05</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.29</td>\n",
       "      <td>3.45</td>\n",
       "      <td>10.35</td>\n",
       "      <td>1.05</td>\n",
       "      <td>8.92</td>\n",
       "      <td>1.63</td>\n",
       "      <td>4.51</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.01</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.38</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>['another', 'silly', 'misconception', 'approac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type  ... OtherP                                         text_final\n",
       "0           0  INFJ  ...   0.17  ['intj', 'moment', 'http', 'sportscenter', 'to...\n",
       "1           1  ENTP  ...   0.32  ['find', 'lack', 'post', 'bore', 'position', '...\n",
       "2           2  INTP  ...   0.44  ['one', 'http', 'course', 'say', 'know', 'bles...\n",
       "3           3  INTJ  ...   0.26  ['intp', 'enjoy', 'conversation', 'day', 'esot...\n",
       "4           4  ENTJ  ...   0.38  ['another', 'silly', 'misconception', 'approac...\n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dly699tpsv9"
   },
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2WPLYTc02lTD"
   },
   "source": [
    "# create 80/20 training/test split of data\n",
    "train_text, test_text, train_label, test_label, train_LIWCS, test_LIWCS = model_selection.train_test_split(MBTI['text_final'],MBTI['type'],LIWCs,test_size=0.2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XBq5WVXv2mSJ"
   },
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(train_label)\n",
    "Test_Y = Encoder.fit_transform(test_label)\n",
    "Train_X = train_text\n",
    "Test_X = test_text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bGA8TEIAxLCg"
   },
   "source": [
    "from tensorflow import io"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "UfsDdVyj1WBa",
    "executionInfo": {
     "status": "error",
     "timestamp": 1635204499199,
     "user_tz": -120,
     "elapsed": 7807,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "b79f0803-a678-4492-80c2-08fb56ce9085"
   },
   "source": [
    "main(Train_X, Train_Y, Test_X, Test_Y, train_LIWCS, test_LIWCS)\n",
    "# must change tokenization.py -> to ::: with ft.io.Gilfe(vocab_file, \"r\") as reader:"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-55-a24cd66a8d8d>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTrain_X\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTrain_Y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTest_X\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTest_Y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_LIWCS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_LIWCS\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;31m# must change tokenization.py -> to ::: with ft.io.Gilfe(vocab_file, \"r\") as reader:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-19-34eb75285ba1>\u001B[0m in \u001B[0;36mmain\u001B[0;34m(train_text, train_label, test_text, test_label, train_liwcs, test_liwcs)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0;31m# Instantiate tokenizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m     \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_tokenizer_from_hub_module\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbert_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;31m# Convert data to InputExample format\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-0c131f97fe3c>\u001B[0m in \u001B[0;36mcreate_tokenizer_from_hub_module\u001B[0;34m(bert_path)\u001B[0m\n\u001B[1;32m      7\u001B[0m     )\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mFullTokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab_file\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvocab_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdo_lower_case\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdo_lower_case\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/bert/tokenization.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, vocab_file, do_lower_case)\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdo_lower_case\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 165\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocab\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload_vocab\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    166\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minv_vocab\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mk\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbasic_tokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBasicTokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdo_lower_case\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdo_lower_case\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/bert/tokenization.py\u001B[0m in \u001B[0;36mload_vocab\u001B[0;34m(vocab_file)\u001B[0m\n\u001B[1;32m    123\u001B[0m   \u001B[0mvocab\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcollections\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOrderedDict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    124\u001B[0m   \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 125\u001B[0;31m   \u001B[0;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mreader\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    126\u001B[0m     \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m       \u001B[0mtoken\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_to_unicode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow' has no attribute 'gfile'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVuM9P-Bxzug",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635204468713,
     "user_tz": -120,
     "elapsed": 370,
     "user": {
      "displayName": "Jocelin pitteloud",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16272144833403683875"
     }
    },
    "outputId": "5457367e-7abb-439f-e51b-4fab998a7cfe"
   },
   "source": [
    "tf.io.gfile"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'tensorflow_core._api.v2.io.gfile' from '/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/io/gfile/__init__.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ]
  }
 ]
}