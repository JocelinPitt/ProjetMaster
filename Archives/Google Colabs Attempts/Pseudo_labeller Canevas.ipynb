{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPYrWE3qpC+rkWP5AVIWURP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QAK4ZTZJLcAt"},"source":["#packages/ autres\n"]},{"cell_type":"code","metadata":{"id":"czuEEe4DLub3"},"source":["import matplotlib.pyplot as plt\n","from matplotlib import ticker\n","\n","%matplotlib inline\n","from sklearn.preprocessing import LabelEncoder\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","import re\n","import json\n","import spacy\n","\n","from numpy import nan"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94BLXts8LqPD"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Btlomom2Li49"},"source":["#traitement données"]},{"cell_type":"markdown","metadata":{"id":"c-q1RD2uC6p0"},"source":["## Importation des données"]},{"cell_type":"code","metadata":{"id":"79drg6jfLflt"},"source":["file = '/content/drive/My Drive/Save_3.txt'\n","\n","with open(file, mode='r+', encoding=\"utf-8\") as script:\n","\tdeveloper = json.load(script)\n","script.close()\n","developer = re.sub(r'null','None',developer)\n","\n","DF1t = pd.DataFrame.from_dict(eval(developer))\n","DF_ScriptPersoFilm = DF1t.T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQK_NPPtDt-D"},"source":["## Prétraitement"]},{"cell_type":"code","metadata":{"id":"cuvYioM7DxWt"},"source":["#Code de transformation des données"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IHEJlOmdDF28"},"source":["## Sélection des données"]},{"cell_type":"code","metadata":{"id":"BDqbZPtgDJV9"},"source":["# Code de sélection des datas\n","max-lenght = 1000\n","min-lenght = 200"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_i6EdLwHB7aW"},"source":["#Gold-Label"]},{"cell_type":"markdown","metadata":{"id":"btq8RNm3DlA0"},"source":["## Importation Gold-Label"]},{"cell_type":"code","metadata":{"id":"IYJCoxBgC1TU"},"source":["#trouver une dataset gold label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8CMVkB7CEiKy"},"source":["#Pseudo Label"]},{"cell_type":"markdown","metadata":{"id":"z1sdAj8gEt53"},"source":["## Def Pseudo Label\n"]},{"cell_type":"code","metadata":{"id":"JPKazt1PEmwA"},"source":["from sklearn.utils import shuffle\n","from sklearn.base import BaseEstimator, RegressorMixin\n","\n","class PseudoLabeler(BaseEstimator, RegressorMixin):\n"," '''\n"," Sci-kit learn wrapper for creating pseudo-lebeled estimators.\n"," '''\n","\n"," def __init__(self, model, unlabled_data, features, target, sample_rate=0.2, seed=42):\n","  '''\n","  @sample_rate - percent of samples used as pseudo-labelled data\n","  from the unlabelled dataset\n","  '''\n","  assert sample_rate <= 1.0, 'Sample_rate should be between 0.0 and 1.0.'\n","\n","  self.sample_rate = sample_rate\n","  self.seed = seed\n","  self.model = model\n","  self.model.seed = seed\n","\n","  self.unlabled_data = unlabled_data\n","  self.features = features\n","  self.target = target\n","\n"," def get_params(self, deep=True):\n","  return {\n","  \"sample_rate\": self.sample_rate,\n","  \"seed\": self.seed,\n","  \"model\": self.model,\n","  \"unlabled_data\": self.unlabled_data,\n","  \"features\": self.features,\n","  \"target\": self.target\n","  }\n","\n"," def set_params(self, **parameters):\n","  for parameter, value in parameters.items():\n","   setattr(self, parameter, value)\n","  return self\n","\n"," def fit(self, X, y):\n","  '''\n","  Fit the data using pseudo labeling.\n","  '''\n","  augemented_train = self.__create_augmented_train(X, y)\n","  self.model.fit(\n","   augemented_train[self.features],\n","   augemented_train[self.target]\n","  )\n","  return self\n","\n"," def __create_augmented_train(self, X, y):\n","  '''\n","  Create and return the augmented_train set that consists\n","  of pseudo-labeled and labeled data.\n","  '''\n","  num_of_samples = int(len(self.unlabled_data) * self.sample_rate)\n","\n","# Train the model and creat the pseudo-labels\n","  self.model.fit(X, y)\n","  pseudo_labels = self.model.predict(self.unlabled_data[self.features])\n","\n","# Add the pseudo-labels to the test set\n","  pseudo_data = self.unlabled_data.copy(deep=True)\n","  pseudo_data[self.target] = pseudo_labels\n","\n","# Take a subset of the test set with pseudo-labels and append in onto\n"," # the training set\n","  sampled_pseudo_data = pseudo_data.sample(n=num_of_samples)\n","  temp_train = pd.concat([X, y], axis=1)\n","  augemented_train = pd.concat([sampled_pseudo_data, temp_train])\n","\n","  return shuffle(augemented_train)\n","\n"," def predict(self, X):\n","  '''\n","  Returns the predicted values.\n","  '''\n","  return self.model.predict(X)\n","\n"," def get_model_name(self):\n","  return self.model.__class__.__name__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2XAlWRdWE0Jj"},"source":["# Deep Model"]},{"cell_type":"markdown","metadata":{"id":"VH0wq7abIwOg"},"source":["def parametres"]},{"cell_type":"code","metadata":{"id":"J4OQS-5XI1qN"},"source":["input_dim = \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ctMruIeSI3xw"},"source":["def model"]},{"cell_type":"code","metadata":{"id":"-qCG0NFVEzKD"},"source":["#Simple model -> ajouter LSTM / CNN\n","model = keras.Sequential([\n","    layers.Dense(64, activation='relu', input_shape=[train_df.shape[1]]),\n","    layers.Dropout(0.3, seed=2),\n","    layers.Dense(64, activation='swish'),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(64, activation='swish'),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(64, activation='swish'),\n","    layers.Dense(1)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zwqtcAqimWH7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4QDJYOhmSJc"},"source":["# exemple CNN\n","from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n","from keras.layers import Reshape, Flatten, Dropout, Concatenate\n","from keras.callbacks import ModelCheckpoint\n","from keras.optimizers import Adam\n","from keras.models import Model\n","from sklearn.model_selection import train_test_split\n","from data_helpers import load_data\n","\n","print('Loading data')\n","x, y, vocabulary, vocabulary_inv = load_data()\n","\n","# x.shape -> (10662, 56)\n","# y.shape -> (10662, 2)\n","# len(vocabulary) -> 18765\n","# len(vocabulary_inv) -> 18765\n","\n","X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42)\n","\n","# X_train.shape -> (8529, 56)\n","# y_train.shape -> (8529, 2)\n","# X_test.shape -> (2133, 56)\n","# y_test.shape -> (2133, 2)\n","\n","\n","sequence_length = x.shape[1] # 56\n","vocabulary_size = len(vocabulary_inv) # 18765\n","embedding_dim = 256\n","filter_sizes = [3,4,5]\n","num_filters = 512\n","drop = 0.5\n","\n","epochs = 100\n","batch_size = 30\n","\n","# this returns a tensor\n","print(\"Creating Model...\")\n","inputs = Input(shape=(sequence_length,), dtype='int32')\n","embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n","reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n","\n","conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n","conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n","conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n","\n","maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n","maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n","maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n","\n","concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n","flatten = Flatten()(concatenated_tensor)\n","dropout = Dropout(drop)(flatten)\n","output = Dense(units=2, activation='softmax')(dropout)\n","\n","# this creates a model that includes\n","model = Model(inputs=inputs, outputs=output)\n","\n","checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n","adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n","\n","model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n","print(\"Traning Model...\")\n","model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wh5Fr-UPJQhQ"},"source":["optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n","\n","model.compile(loss=tf.keras.losses.MeanSquaredError(),\n","              optimizer=optimizer,\n","              metrics=['mae'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"THtfgg1aJRK6"},"source":["history = model.fit(\n","    train_df, train_labels,\n","    epochs=70, validation_split=0.2\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AsfJMDpwIuWJ"},"source":["## Compile model\n","\n"]},{"cell_type":"code","metadata":{"id":"FV6YXJUmI9ux"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ulKANJ2AQjM0"},"source":["#Analyse"]},{"cell_type":"code","metadata":{"id":"dZVNQknbQmea"},"source":["model_history = pd.DataFrame(history.history)\n","model_history['epoch'] = history.epoch\n","\n","fig, ax = plt.subplots(figsize=(14,8))\n","num_epochs = model_history.shape[0]\n","ax.plot(np.arange(0, num_epochs), model_history[\"mae\"], \n","        label=\"Training MAE\", lw=3, color='#f4b400')\n","ax.plot(np.arange(0, num_epochs), model_history[\"val_mae\"], \n","        label=\"Validation MAE\", lw=3, color='#0f9d58')\n","ax.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]}]}